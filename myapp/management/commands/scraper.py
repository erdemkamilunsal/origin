import time
import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timedelta
import pytz
from concurrent.futures import ThreadPoolExecutor, as_completed
from unidecode import unidecode
from django.core.management.base import BaseCommand
from myapp.models import MostSharedContent, ScraperLog, LatestData, Latest7Days

def update_scraper_log():
    """Scraper log'unu gÃ¼nceller. EÄŸer log kaydÄ± varsa gÃ¼nceller, yoksa yeni bir kayÄ±t oluÅŸturur."""
    log_entry = ScraperLog.objects.first()
    if log_entry:
        log_entry.save()  # `auto_now=True` olduÄŸu iÃ§in otomatik olarak gÃ¼ncellenir
    else:
        ScraperLog.objects.create()  # Ä°lk defa Ã§alÄ±ÅŸÄ±yorsa yeni bir kayÄ±t oluÅŸtur

def fetch_data(session, url):
    """URL'den veri Ã§eker ve JSON formatÄ±nda dÃ¶ner."""
    try:
        response = session.get(url, timeout=10)  # 10 saniyelik zaman aÅŸÄ±mÄ± ile istek atar
        if response.status_code == 200:  # BaÅŸarÄ±lÄ± bir yanÄ±t alÄ±ndÄ±ysa
            return json.loads(response.text)  # JSON verisini Python dict'ine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
    except Exception as e:
        print(f"{url} iÃ§in hata oluÅŸtu: {e}")  # Hata durumunda log yazdÄ±rÄ±r
    return None  # Hata durumunda None dÃ¶ner

def fetch_last_7_days(session, base_url, selective, channels_to_find, today):
    """Son 7 gÃ¼nlÃ¼k verileri Ã§eker ve veritabanÄ±na kaydeder."""
    seven_days_ago = today - timedelta(days=7)  # 7 gÃ¼n Ã¶ncesini hesapla
    deleted_count, _ = Latest7Days.objects.filter(
        created_time__lte=seven_days_ago.date()  # 7 gÃ¼nden eski verileri sil
    ).delete()
    print(f"ğŸ—‘ {deleted_count} adet eski kayÄ±t (7 gÃ¼nden eski) temizlendi.")

    for i in range(7):  # Son 7 gÃ¼n iÃ§in dÃ¶ngÃ¼
        date_start_dt = today - timedelta(days=i)  # Ä°lgili gÃ¼nÃ¼n baÅŸlangÄ±Ã§ tarihi
        record_date = date_start_dt.date()  # Sadece tarih kÄ±smÄ±nÄ± al
        date_end_dt = date_start_dt + timedelta(days=1)  # Bir sonraki gÃ¼nÃ¼n baÅŸlangÄ±cÄ±
        since_str = date_start_dt.strftime("%d.%m.%Y 00:00").replace(" ", "%20")  # URL iÃ§in tarih formatÄ±
        until_str = date_end_dt.strftime("%d.%m.%Y 00:00").replace(" ", "%20")  # URL iÃ§in tarih formatÄ±

        for channel in channels_to_find:  # Her kanal iÃ§in veri Ã§ek
            extra_url = (
                f"https://{base_url}.ebrandvalue.com/industries/{selective}/social/posts/"
                f"?path_param=posts&source={channel}&since={since_str}&until={until_str}"
            )
            print(f"{extra_url} verisi Ã§ekiliyor...")

            response = session.get(extra_url)
            if response.status_code != 200:  # Ä°stek baÅŸarÄ±sÄ±zsa
                print(f"{since_str} - {base_url} iÃ§in {channel} verisi bulunamadÄ± veya geÃ§ersiz.")
                continue

            parsed_data = json.loads(response.text)  # JSON verisini parse et
            paging_data = parsed_data.get('paging', {})  # Paging verisini al
            author_count = paging_data.get("authors", 0)  # Yazar sayÄ±sÄ±
            content_count = paging_data.get("total", 0)  # Ä°Ã§erik sayÄ±sÄ±

            # Eski verileri sil
            Latest7Days.objects.filter(
                source_category=base_url,
                selective_part=selective,
                source=channel,
                created_time=record_date
            ).delete()

            # Yeni veriyi ekle
            Latest7Days.objects.create(
                source_category=base_url,
                selective_part=selective,
                source=channel,
                created_time=record_date,
                author=author_count,
                total=content_count
            )
            print(f"âœ… {base_url} - {selective} - {channel} iÃ§in yeni veri eklendi.")

def fetch_yesterday_data(session, base_url, selective, channels_to_find, today):
    """Her kanal iÃ§in en son veriyi Ã§eker ve veritabanÄ±na kaydeder."""
    for channel in channels_to_find:  # Her kanal iÃ§in veri Ã§ek
        current_url = f"https://{base_url}.ebrandvalue.com/industries/{selective}/social/posts/?path_param=posts&source={channel}&start=0"
        print(f"{current_url} verisi Ã§ekiliyor...")

        response = session.get(current_url)
        if response.status_code != 200:  # Ä°stek baÅŸarÄ±sÄ±zsa
            print(f"{selective} kategorisinde {channel} kanalÄ± iÃ§in veri bulunamadÄ± veya geÃ§ersiz.")
            continue

        parsed_data = json.loads(response.text)  # JSON verisini parse et
        all_data = parsed_data.get('data', [])  # TÃ¼m verileri al

        if all_data:
            entry = all_data[0]  # Ä°lk iÃ§eriÄŸi al
            body = entry['content'].get('body', None)
            body = unidecode(body) if body else None  # Unicode karakterleri normalize et

            # Bu kanal ve kategori iÃ§in eski verileri sil
            LatestData.objects.filter(
                source_category=base_url,
                selective_part=selective,
                source=channel
            ).delete()

            # Yeni veriyi ekle
            LatestData.objects.create(
                source_category=base_url,
                author_name=entry['author'].get('name', None),
                author_nick=entry['author'].get('nick', None),
                author_follower_count=entry['author'].get('follower_count', None),
                body=body,
                source=entry.get('source', None),
                link=entry['content'].get('link', None),
                created_time=entry['content'].get('create_time', None),
                selective_part=selective
            )
            print(f"âœ… {selective} - {channel} iÃ§in yeni veri eklendi.")
        else:
            print(f"{channel} kanalÄ± iÃ§in {base_url} - {selective} kategorisinde veri bulunamadÄ±.")

def fetch_most_shared(session, base_url, selective, kanallar):
    """Bir Ã¶nceki gÃ¼nÃ¼n en Ã§ok paylaÅŸÄ±lan 10 iÃ§eriÄŸini Ã§eker ve veritabanÄ±na kaydeder."""
    # Ä°stanbul saat dilimi
    istanbul_tz = pytz.timezone('Europe/Istanbul')
    today = datetime.now(istanbul_tz)

    # Bir Ã¶nceki gÃ¼nÃ¼n baÅŸlangÄ±Ã§ tarihi (00:00)
    start_date = today - timedelta(days=1)
    start_date = start_date.replace(hour=0, minute=0, second=0, microsecond=0)

    # BugÃ¼nÃ¼n baÅŸlangÄ±Ã§ tarihi (00:00)
    end_date = today.replace(hour=0, minute=0, second=0, microsecond=0)

    # Tarihleri string formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rme
    since_str = start_date.strftime("%d.%m.%Y 00:00").replace(" ", "%20")
    until_str = end_date.strftime("%d.%m.%Y 00:00").replace(" ", "%20")

    for channel in kanallar:  # Her kanal iÃ§in veri Ã§ek
        # URL oluÅŸturma
        current_url = f"https://{base_url}.ebrandvalue.com/industries/{selective}/social/posts/?path_param=posts&source={channel}&most_shared=true&since={since_str}&until={until_str}"
        print(f"{current_url} verisi Ã§ekiliyor...")

        response = session.get(current_url)
        if response.status_code != 200:  # Ä°stek baÅŸarÄ±sÄ±zsa
            print(f"{selective} kategorisinde {channel} iÃ§in veri bulunamadÄ± veya geÃ§ersiz.")
            continue

        parsed_data = json.loads(response.text)  # JSON verisini parse et
        all_data = parsed_data.get('data', [])  # TÃ¼m verileri al

        # Ä°lk 10 elemanÄ± al
        all_data = all_data[:10]

        # Bu kanal ve kategori iÃ§in eski verileri sil
        MostSharedContent.objects.filter(
            source_category=base_url,
            selective_part=selective,
            source=channel,
            created_time=start_date.date()  # Bir Ã¶nceki gÃ¼nÃ¼n tarihi
        ).delete()

        # Yeni verileri ekle
        for entry in all_data:
            body = entry['content'].get('body', None)
            body = unidecode(body) if body else None  # Unicode karakterleri normalize et

            # Veriyi oluÅŸtur
            post_data = {
                "avatar": entry['author'].get('avatar', None),
                "follower_count": entry['author'].get('follower_count', None),
                "following_count": entry['author'].get('following_count', None),
                "name": entry['author'].get('name', None),
                "nick": entry['author'].get('nick', None),
                "brands": entry.get('brands', None),
                "content": body,
                "comment_count": entry['content'].get('comment_count', None),
                "favourite_count": entry['content'].get('favourite_count', None),
                "create_time": entry['content'].get('create_time', None),
                "link": entry['content'].get('link', None),
                "quote_count": entry['content'].get('quote_count', None),
                "reply_content": entry['content'].get('reply_content', None),
                "retweet_count": entry['content'].get('retweet_count', None),
                "view_count": entry['content'].get('view_count', None),
                "source": entry.get('source', None),
                "selective_part": selective,
                "created_time": start_date.date(),  # Bir Ã¶nceki gÃ¼nÃ¼n tarihi
                "source_category": base_url
            }

            # Yeni veriyi ekle
            MostSharedContent.objects.create(**post_data)
            print(f"âœ… {selective} - {channel} iÃ§in yeni veri eklendi: {post_data['name']} - {post_data['create_time']}")

class Command(BaseCommand):
    help = "Sosyal medya verilerini Ã§ekip kaydeder."

    def handle(self, *args, **kwargs):
        start_time = time.time()  # BaÅŸlangÄ±Ã§ zamanÄ±
        base_urls = {
            "finans": ["corporate","selective", "primary"],
            "mey": ["primary", "selective"],
            "snacks-tr": ["corporate", "primary", "selective", "corprimary", "pladis_categories"],
            "mey-international": ["primary"]
        }
        channels_to_find = {
            "twitter", "facebook", "facebook_page_comment", "facebook_page_like",
            "youtube", "youtube_shorts", "instagram", "instagram_comment",
            "tiktok", "pinterest", "rss", "apple_app_store_comment",
            "google_play_store_comment", "linkedin", "donanimhaber", "eksi_sozluk",
            "inci_sozluk", "sikayetvar", "uludag_sozluk"
        }
        kanallar = {"twitter", "facebook", "youtube", "instagram"}  # Sosyal medya kanallarÄ±
        istanbul_tz = pytz.timezone('Europe/Istanbul')  # Ä°stanbul zaman dilimi
        today = datetime.now(istanbul_tz)  # BugÃ¼nÃ¼n tarihi

        with requests.Session() as session:  # Oturum baÅŸlat
            for base_url, selective_parts in base_urls.items():  # Her bir URL ve kategori iÃ§in
                # GiriÅŸ iÅŸlemleri
                login_page_url = f"https://{base_url}.ebrandvalue.com/accounts/login/?next=/"
                login_page = session.get(login_page_url)
                if login_page.status_code != 200:  # GiriÅŸ sayfasÄ±na ulaÅŸÄ±lamazsa
                    print(f"{base_url} iÃ§in giriÅŸ sayfasÄ±na ulaÅŸÄ±lamadÄ±.")
                    continue

                soup = BeautifulSoup(login_page.text, "html.parser")  # HTML'i parse et
                csrf_token = soup.find("input", {"name": "csrfmiddlewaretoken"})["value"]  # CSRF token'Ä± al
                payload = {"username": "erdem.unsal", "password": "eu123", "csrfmiddlewaretoken": csrf_token}  # GiriÅŸ bilgileri
                headers = {"User-Agent": "Mozilla/5.0", "Referer": login_page_url}  # Ä°stek baÅŸlÄ±klarÄ±
                response = session.post(login_page_url, data=payload, headers=headers)  # GiriÅŸ isteÄŸi

                if response.status_code != 200 or "dashboard" not in response.text:  # GiriÅŸ baÅŸarÄ±sÄ±zsa
                    print(f"{base_url} iÃ§in giriÅŸ baÅŸarÄ±sÄ±z!")
                    continue

                # Veri Ã§ekme iÅŸlemleri
                for selective in selective_parts:
                    #fetch_yesterday_data(session, base_url, selective, channels_to_find, today)  # DÃ¼nÃ¼n verilerini Ã§ek
                    fetch_last_7_days(session, base_url, selective, channels_to_find, today)  # Son 7 gÃ¼nlÃ¼k verileri Ã§ek
                    #fetch_most_shared(session, base_url, selective, kanallar)  # En popÃ¼ler iÃ§erikleri Ã§ek

        end_time = time.time()  # BitiÅŸ zamanÄ±
        print(f"Scraper tamamlandÄ±! Toplam sÃ¼re: {end_time - start_time:.2f} saniye.")  # Toplam sÃ¼reyi yazdÄ±r